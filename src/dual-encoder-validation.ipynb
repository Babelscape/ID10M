{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    " \n",
    "from pprint import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../resources/bio_format\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Spanish\"\n",
    "data_file = f\"{language.lower()}/{language.lower()}.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {}\n",
    "idx = 0\n",
    "tokens = 0\n",
    "\n",
    "count = 0\n",
    "with open(data_file) as f:\n",
    "    sentence = \"\"\n",
    "    expression = \"\"\n",
    "    prev_tag = \"O\"\n",
    "    \n",
    "    for line in f:\n",
    "        if line == \"\\n\":\n",
    "            if expression != \"\":\n",
    "                data[idx] = {\"expression\": expression.strip(),\n",
    "                            \"text\": sentence.strip(),\n",
    "                            \"idiomatic\": True}\n",
    "            else:\n",
    "                data[idx] = {\"expression\": expression.strip(),\n",
    "                            \"text\": sentence.strip(),\n",
    "                            \"idiomatic\": False}\n",
    "                \n",
    "            idx += 1\n",
    "            sentence = \"\"\n",
    "            expression = \"\"\n",
    "            prev_tag = \"O\"\n",
    "\n",
    "        else:\n",
    "            tokens += 1\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if len(line)==2:\n",
    "                token = line[0]\n",
    "                tag = line[1]\n",
    "                sentence += token\n",
    "                if tag == \"B-IDIOM\":\n",
    "                    expression += token\n",
    "                    prev_tag = \"B-IDIOM\"\n",
    "                elif tag == \"I-IDIOM\" and prev_tag == \"B-IDIOM\":\n",
    "                    expression += token\n",
    "                    prev_tag = \"I-IDIOM\"\n",
    "                elif tag == \"I-IDIOM\" and prev_tag == \"I-IDIOM\":\n",
    "                    expression += token\n",
    "                    prev_tag = \"I-IDIOM\"\n",
    "                elif tag==\"O\":\n",
    "                    prev_tag = \"O\"\n",
    "            else:\n",
    "                print(\"count \" + str(count))\n",
    "                count+=1\n",
    "    \n",
    "print(len(data)) \n",
    "print(tokens)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2 #we set a seed for having replicability of results\n",
    " \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    " \n",
    "bert_config = BertConfig.from_pretrained(model_name, output_hidden_states=True)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name, config=bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdiomDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 tokenizer: BertTokenizer,\n",
    "                 device=\"cuda\",\n",
    "                ) -> None:\n",
    "        \n",
    "        self.encoded_data = []\n",
    "    \n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.__init_encoded_data(self.dataset)\n",
    " \n",
    "    def __init_encoded_data(self, dataset):\n",
    "                \n",
    "        for idx in tqdm(self.dataset):\n",
    "            e = data[idx][\"expression\"]\n",
    "            context_tmp = data[idx][\"text\"]\n",
    "            if e!=context_tmp:\n",
    "                context = context_tmp[:context_tmp.find(e)] + context_tmp[context_tmp.find(e)+len(e)-1:]\n",
    "            else:\n",
    "                context = context_tmp\n",
    "        \n",
    "            if len(context)<300:\n",
    "                tokenized_e = torch.tensor(self.tokenize_mention(e, self.tokenizer, True))\n",
    "                tokenized_context = torch.tensor(self.tokenize_mention(context, self.tokenizer, True))\n",
    "\n",
    "                if e!=\"\":\n",
    "                    self.encoded_data.append((idx,\n",
    "                                            e,\n",
    "                                            context_tmp,\n",
    "                                            tokenized_e,\n",
    "                                            tokenized_context))\n",
    "\n",
    "\n",
    "            \n",
    "     \n",
    "    def tokenize_mention(self, sent, tokenizer, special_tokens):\n",
    "        encoded_sentence = tokenizer.encode(sent, add_special_tokens = special_tokens)\n",
    "        return encoded_sentence\n",
    "    \n",
    "    def tokenize_description(self, sent, tokenizer, window):\n",
    "        encoded_sentence = tokenizer.encode(sent, add_special_tokens = True)\n",
    "        return encoded_sentence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    " \n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.encoded_data[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(BERT, self).__init__()\n",
    "        pprint(params)\n",
    " \n",
    "        self.hparams = hparams\n",
    " \n",
    "        self.expression_encoder = bert_model #BertModel.from_pretrained(model_name, config=bert_config)\n",
    "        self.context_encoder = bert_model #BertModel.from_pretrained(model_name, config=bert_config)\n",
    "\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "        \n",
    "        self.dropout = nn.Dropout(hparams.dropout)\n",
    "        \n",
    "        #self.pooling = nn.AvgPool1d(3, stride=3)\n",
    "                \n",
    "        for param in self.expression_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for param in self.context_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "  \n",
    "\n",
    "    def forward(self, expression, context, mask1, mask2):\n",
    "\n",
    "        embedding_expression = self.expression_encoder.forward(expression.cuda(), mask1.cuda())[0]\n",
    "        embedding_expression = torch.sum(embedding_expression, 1)\n",
    "        #embedding_expression = embedding_expression[:,0,:].squeeze(1)\n",
    "            \n",
    "        embedding_context = self.context_encoder.forward(context.cuda(), mask2.cuda())[0] #320x64x768\n",
    "        #embedding_context = torch.mean(embedding_context, 1)\n",
    "        embedding_context = embedding_context[:,0,:].squeeze(1) #320x768\n",
    "            \n",
    "        similarities = self.cosine_similarity(embedding_expression, embedding_context) \n",
    "                        \n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer with cross entropy\n",
    "\n",
    "import math \n",
    "\n",
    "class Predict():\n",
    "    def __init__(self,\n",
    "                model:nn.Module, \n",
    "                tokenizer):\n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    " \n",
    "    def padding_mask(self, batch):\n",
    "        padding = torch.ones_like(batch)\n",
    "        padding[batch == 0] = 0\n",
    "        padding = padding.type(torch.int64)\n",
    "        return padding\n",
    "    \n",
    "    def normalize(self, m):\n",
    "        row_min, _ = m.min(dim=1, keepdim=True)\n",
    "        row_max, _ = m.max(dim=1, keepdim=True)\n",
    "        return (m - row_min) / (row_max - row_min)\n",
    " \n",
    "    def predict(self,\n",
    "            dataset:Dataset):\n",
    "        \n",
    "        print(\"\\nPredicting...\")\n",
    "                 \n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        not_idiomatic = []\n",
    "        not_idiomatic = []\n",
    "    \n",
    "        for ids, e, text, expressions, contexts in tqdm(dataset):\n",
    "            mask1 = self.padding_mask(expressions)\n",
    "            mask2 = self.padding_mask(contexts)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                similarities = self.model(expressions, contexts, mask1, mask2)\n",
    "\n",
    "            if language not in [\"Chinese\", \"Japanese\"] and similarities.item()>0.4 and len(text[0].split())>4:\n",
    "                not_idiomatic.append(ids)\n",
    "                print(e, \"\\n\", text)\n",
    "                print(similarities.item())\n",
    "                print(\"\\n\\n\\n\")\n",
    "            elif language in [\"Chinese\", \"Japanese\"] and similarities.item()>0.55:\n",
    "                not_idiomatic.append(ids)\n",
    "                print(e, \"\\n\", text)\n",
    "                print(similarities.item())\n",
    "                print(\"\\n\\n\\n\")\n",
    "        \n",
    "        return not_idiomatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Datasets and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = IdiomDataset(data, bert_tokenizer)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collate(elems: tuple) -> tuple:\n",
    "    ids, e, texts, expressions, contexts = list(zip(*elems))\n",
    "    \n",
    "    pad_expressions = pad_sequence(expressions, batch_first=True, padding_value=0)\n",
    "    pad_contexts = pad_sequence(contexts, batch_first=True, padding_value=0)\n",
    " \n",
    "    return ids, e, texts, pad_expressions.to(torch.int64), pad_contexts.to(torch.int64)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter, Training and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams():\n",
    "    dropout = 0.25\n",
    "    \n",
    "params = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "el_model = BERT(params).cuda()\n",
    "el_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predict(model = el_model,\n",
    "                    tokenizer = bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_idiomatic = predictor.predict(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in not_idiomatic:\n",
    "    if language not in [\"Chinese\", \"Japanese\"] and len(data[idx[0]][\"text\"].split(\" \"))>8:\n",
    "        data[idx[0]][\"idiomatic\"] = False\n",
    "    elif language in [\"Chinese\", \"Japanese\"]:\n",
    "        data[idx[0]][\"idiomatic\"] = False\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "with open(f\"../json_format/{language.lower()}/{language}_preannotations_dual.json\", \"w\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f64be793538f7fe230f350828c9baf03d97c4df0981f52e8388f53f367f4a42"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
